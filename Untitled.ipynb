{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5beeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "# import torch.nn as nn \n",
    "import time\n",
    "import math\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "from net import *\n",
    "from utils import *\n",
    "from tracker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b271fd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_m = Inception3_M()\n",
    "backbone_m.update_params()\n",
    "backbone_q = Inception3_Q()\n",
    "backbone_q.update_params()\n",
    "neck_m = AdjustLayer()\n",
    "neck_m.update_params()\n",
    "neck_q = AdjustLayer()\n",
    "neck_q.update_params()\n",
    "head = STMHead()\n",
    "head.update_params()\n",
    "\n",
    "model = STMTrack(backbone_m, backbone_q, neck_m, neck_q, head)\n",
    "# model.update_params()\n",
    "\n",
    "# Convert BatchNorm to SyncBatchNorm \n",
    "# task_model = convert_model(task_model)\n",
    "model_file = \"new-epoch-19.pkl\"\n",
    "\n",
    "# model_file = \"epoch-19.pkl\"\n",
    "model_state_dict = torch.load(model_file,\n",
    "                        map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# model.load_state_dict(model_state_dict['model_state_dict'])\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345ade3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f71a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"car1_s\"\n",
    "path_gt = \"/media/meysam/hdd/dataset/Dataset_UAV123/UAV123/anno/UAV123/car1_s.txt\" \n",
    "img_files_path = glob.glob(\"/media/meysam/hdd/dataset/Dataset_UAV123/UAV123/data_seq/UAV123/car1_s/*\")\n",
    "img_files_path.sort()\n",
    "\n",
    "img_files = []\n",
    "for i in img_files_path:\n",
    "        frame = cv2.imread(i, cv2.IMREAD_COLOR)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_files.append(frame)\n",
    "\n",
    "my_file = open(path_gt)\n",
    "\n",
    "line = my_file.readline()\n",
    "line = [int(l) for l in line[:-1].split(',')]\n",
    "my_file.close()\n",
    "\n",
    "\n",
    "box = line\n",
    "frame_num = len(img_files)\n",
    "boxes = np.zeros((frame_num, 4))\n",
    "boxes[0] = box\n",
    "times = np.zeros(frame_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b234a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init(self, im, state):\n",
    "im = img_files[0]\n",
    "state = box\n",
    "_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dff74100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([764., 494., 215., 272.], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2755c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = state  # bbox in xywh format is given for initialization in case of tracking\n",
    "box = xywh2cxywh(rect)\n",
    "target_pos, target_sz = box[:2], box[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c40d824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([871. , 629.5], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05402fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([215., 272.], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c198e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_state['im_h'] = im.shape[0]\n",
    "_state['im_w'] = im.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80a9e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyper_params = dict(\n",
    "        total_stride=8,\n",
    "        score_size=25,\n",
    "        score_offset=-1,\n",
    "        test_lr=0.95,\n",
    "        penalty_k=0.04,\n",
    "        window_influence=0.21,\n",
    "        windowing=\"cosine\",\n",
    "        m_size=289,\n",
    "        q_size=289,\n",
    "        min_w=10,\n",
    "        min_h=10,\n",
    "        phase_memorize=\"memorize\",\n",
    "        phase_track=\"track\",\n",
    "        corr_fea_output=False,\n",
    "        num_segments=4,\n",
    "        confidence_threshold=0.6,\n",
    "        gpu_memory_threshold=-1,\n",
    "        search_area_factor=4.0,\n",
    "        visualization=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53626c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0deb10f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hanning() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10494/3037472019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscore_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhanning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhanning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# window = window.reshape(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: hanning() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "score_size = 25 \n",
    "window = np.outer(np.hanning(score_size,dtype=np.float32), np.hanning(score_size,dtype=np.float32))\n",
    "# window = window.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8eb2e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "541dc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_chans = (np.mean(im[..., 0]), np.mean(im[..., 1]), np.mean(im[..., 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a8a736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_area = np.prod(target_sz * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0b7648b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935680.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6827b75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 860., 1088.], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sz*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54897531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im_h': 720, 'im_w': 1280, 'target_scale': 3.347077979281238}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state['target_scale'] = math.sqrt(search_area) / 289\n",
    "_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63ddd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "_state['base_target_sz'] = target_sz / _state['target_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04e595b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im_h': 720,\n",
       " 'im_w': 1280,\n",
       " 'target_scale': 3.347077979281238,\n",
       " 'base_target_sz': array([64.23513, 81.26491], dtype=float32)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28070ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_size = 289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21145e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_m = math.sqrt(np.prod(target_sz) / np.prod(_state['base_target_sz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e27e22a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3470781822043065"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78ac6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = img_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "499b5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_m_crop, real_scale = get_crop_single(im, target_pos, scale_m, m_size, avg_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f39f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2982456140350877"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7ecee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 289, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_m_crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af90fbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f17cfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_m = np.concatenate([np.array([(m_size - 1) / 2, (m_size - 1) / 2]),\n",
    "                                    target_sz * real_scale], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5c53dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_numpy(im: np.ndarray, pos: np.ndarray, sample_sz: np.ndarray, output_sz: np.ndarray = None,\n",
    "                   mode: str = 'constant', avg_chans=(0, 0, 0), max_scale_change=None):\n",
    "    \"\"\"Sample an image patch.\n",
    "\n",
    "    args:\n",
    "        im: Image\n",
    "        pos: center position of crop\n",
    "        sample_sz: size to crop\n",
    "        output_sz: size to resize to\n",
    "        mode: how to treat image borders: 'replicate' (default), 'inside' or 'inside_major'\n",
    "        max_scale_change: maximum allowed scale change when using 'inside' and 'inside_major' mode\n",
    "    \"\"\"\n",
    "\n",
    "    # if mode not in ['replicate', 'inside']:\n",
    "    #     raise ValueError('Unknown border mode \\'{}\\'.'.format(mode))\n",
    "\n",
    "    # copy and convert\n",
    "    posl = pos.astype(np.int32).copy()\n",
    "\n",
    "    # Get new sample size if forced inside the image\n",
    "#     if mode == 'inside' or mode == 'inside_major':\n",
    "#         pad_mode = 'replicate'\n",
    "#         # im_sz = torch.tensor([im.shape[2], im.shape[3]], device=im.device)\n",
    "#         # shrink_factor = (sample_sz.float() / im_sz)\n",
    "#         im_sz = np.array([im.shape[0], im.shape[1]])\n",
    "#         shrink_factor = (sample_sz.astype(np.float) / im_sz)\n",
    "#         if mode == 'inside':\n",
    "#             shrink_factor = shrink_factor.max()\n",
    "#         elif mode == 'inside_major':\n",
    "#             shrink_factor = shrink_factor.min()\n",
    "#         shrink_factor.clamp_(min=1, max=max_scale_change)\n",
    "#         # sample_sz = (sample_sz.float() / shrink_factor).long()\n",
    "#         sample_sz = (sample_sz.astype(np.float) / shrink_factor).astype(np.int32)\n",
    "\n",
    "    # Compute pre-downsampling factor\n",
    "    if output_sz is not None:\n",
    "        # resize_factor = torch.min(sample_sz.float() / output_sz.float()).item()\n",
    "        resize_factor = np.min(sample_sz.astype(np.float32) / output_sz.astype(np.float32)).item()\n",
    "        df = int(max(int(resize_factor - 0.1), 1))\n",
    "    else:\n",
    "        df = int(1)\n",
    "\n",
    "    # sz = sample_sz.float() / df  # new size\n",
    "    sz = sample_sz.astype(np.float32) / df\n",
    "\n",
    "    # Do downsampling\n",
    "    if df > 1:\n",
    "        os = posl % df  # offset\n",
    "        posl = (posl - os) // df  # new position\n",
    "        im2 = im[os[0].item()::df, os[1].item()::df, :]  # downsample\n",
    "    else:\n",
    "        im2 = im\n",
    "\n",
    "    # compute size to crop\n",
    "    # szl = torch.max(sz.round(), torch.tensor([2.0], dtype=sz.dtype, device=sz.device)).long()\n",
    "    szl = np.maximum(np.round(sz), 2.0).astype(np.int32)\n",
    "\n",
    "    # Extract top and bottom coordinates\n",
    "    tl = posl - (szl - 1) // 2\n",
    "    br = posl + szl // 2 + 1\n",
    "\n",
    "    # Shift the crop to inside\n",
    "#     if mode == 'inside' or mode == 'inside_major':\n",
    "#         # im2_sz = torch.LongTensor([im2.shape[2], im2.shape[3]])\n",
    "#         # shift = (-tl).clamp(0) - (br - im2_sz).clamp(0)\n",
    "#         im2_sz = np.array([im2.shape[0], im2.shape[1]], dtype=np.int32)\n",
    "#         shift = np.clip(-tl, 0) - np.clip(br - im2_sz, 0)\n",
    "#         tl += shift\n",
    "#         br += shift\n",
    "\n",
    "#         # outside = ((-tl).clamp(0) + (br - im2_sz).clamp(0)) // 2\n",
    "#         # shift = (-tl - outside) * (outside > 0).long()\n",
    "#         outside = (np.clip(-tl, 0) - np.clip(br - im2_sz, 0)) // 2\n",
    "#         shift = (-tl - outside) * (outside > 0).astype(np.int32)\n",
    "#         tl += shift\n",
    "#         br += shift\n",
    "\n",
    "#         # Get image patch\n",
    "#         # im_patch = im2[...,tl[0].item():br[0].item(),tl[1].item():br[1].item()]\n",
    "\n",
    "    crop_xyxy = np.array([tl[1], tl[0], br[1], br[0]])\n",
    "    # warpAffine transform matrix\n",
    "    M_13 = crop_xyxy[0]\n",
    "    M_23 = crop_xyxy[1]\n",
    "    M_11 = (crop_xyxy[2] - M_13) / (output_sz[0] - 1)\n",
    "    M_22 = (crop_xyxy[3] - M_23) / (output_sz[1] - 1)\n",
    "    mat2x3 = np.array([\n",
    "        M_11,\n",
    "        0,\n",
    "        M_13,\n",
    "        0,\n",
    "        M_22,\n",
    "        M_23,\n",
    "    ]).reshape(2, 3)\n",
    "    im_patch = cv2.warpAffine(im2,\n",
    "                              mat2x3, (output_sz[0], output_sz[1]),\n",
    "                              flags=(cv2.INTER_LINEAR | cv2.WARP_INVERSE_MAP),\n",
    "                              borderMode=cv2.BORDER_CONSTANT,\n",
    "                              borderValue=tuple(map(int, avg_chans)))\n",
    "    # Get image coordinates\n",
    "    patch_coord = df * np.concatenate([tl, br]).reshape(1, 4)\n",
    "    scale = output_sz / (np.array([br[1] - tl[1] + 1, br[0] - tl[0] + 1]) * df)\n",
    "    return im_patch, patch_coord, scale\n",
    "\n",
    "def get_crop_single(im: np.ndarray, target_pos: np.ndarray, target_scale: float, output_sz: int, avg_chans: tuple):\n",
    "    pos = target_pos[::-1]\n",
    "    output_sz = np.array([output_sz, output_sz])\n",
    "    sample_sz = target_scale * output_sz\n",
    "    im_patch, _, scale_x = get_crop_numpy(im, pos, sample_sz, output_sz, avg_chans=avg_chans)\n",
    "    return im_patch, scale_x[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1aeb72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_m_crop, real_scale = get_crop_single(im, target_pos, scale_m, m_size, avg_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8716135",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scale = scale_m\n",
    "output_sz = m_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13b6163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = target_pos[::-1]\n",
    "output_sz = np.array([output_sz, output_sz])\n",
    "sample_sz = target_scale * output_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b8145d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([629.5, 871. ], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ce3cf99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2982456140350877"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d3539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
